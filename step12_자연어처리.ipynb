{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 처리 text 분석을 위한 환경세팅\n",
    "\n",
    "step01 자연어 처리 라이브러리\n",
    "\n",
    "pip install nltk\n",
    "\n",
    "<hr>\n",
    "\n",
    "step02 한국어 분석을 위한 추가 라이브러리 세팅\n",
    "\n",
    "1. 접속 : https://www.lfd.uci.edu/~gohlke/pythonlibs/#jpype\n",
    "2. 다운로드 : JPype1‑1.2.0‑cp38‑cp38‑win_amd64.whl(파이썬 버전 체크 필)\n",
    "3. 설치 whl 파일이 있는 영역에서 다음 명령어 입력해서 실행\n",
    "\n",
    "4. 한글 text 분석 모듈 설치 : pip install KoNLPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'going', 'to', 'Seoul', ',', 'Korea', '.']\n",
      "[('I', 'PRP'), ('am', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('Seoul', 'NNP'), (',', ','), ('Korea', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens =  word_tokenize(\"I am going to Seoul, Korea.\")\n",
    "print( tokens )\n",
    "print( nltk.pos_tag(tokens) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n",
      "['항공기', '체계', '종합', '개발', '경험']\n",
      "['날카로운 분석', '날카로운 분석과 신뢰감', '날카로운 분석과 신뢰감 있는 진행', '분석', '신뢰', '진행']\n",
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나욬', 'Noun'), ('ㅋㅋ', 'KoreanParticle')]\n",
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나요', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n",
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되다', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt   #Twitter\n",
    "okt = Okt()\n",
    "print(okt.morphs('단독입찰보다 복수입찰의 경우'))\n",
    "#['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n",
    "print(okt.nouns('유일하게 항공기 체계 종합개발 경험을 갖고 있는 KAI는'))\n",
    "#['항공기', '체계', '종합', '개발', '경험']\n",
    "print(okt.phrases('날카로운 분석과 신뢰감 있는 진행으로'))\n",
    "#['날카로운 분석', '날카로운 분석과 신뢰감', '날카로운 분석과 신뢰감 있는 진행', '분석', '신뢰', '진행']\n",
    "print(okt.pos('이것도 되나욬ㅋㅋ'))\n",
    "#[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나욬', 'Noun'), ('ㅋㅋ', 'KoreanParticle')]\n",
    "print(okt.pos('이것도 되나욬ㅋㅋ', norm=True))\n",
    "#[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나요', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n",
    "print(okt.pos('이것도 되나욬ㅋㅋ', norm=True, stem=True))\n",
    "#[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되다', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**분석된 내용을 확장자가 m 이라는 파일로 주로 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "readFp = open(\"data.txt\", \"r\", encoding=\"utf-8\")\n",
    "wakati_file = \"text.m\"\n",
    "writeFp = open(wakati_file, \"w\", encoding=\"utf-8\")\n",
    "# 형태소 분석\n",
    "twitter = Okt()\n",
    "i = 0\n",
    "while True:\n",
    "    line = readFp.readline()\n",
    "    if not line: break\n",
    "    # if i % 100 == 0:\n",
    "    #     print(\"current - \" + str(i)) #100라인에 한번씩 출력\n",
    "    # i += 1\n",
    "    malist = twitter.pos(line, norm=True, stem=True)\n",
    "    for word in malist:\n",
    "                   # 조사/어미/구두점 등은 대상에서 제외\n",
    "        #if not word[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "        #    writeFp.write(word[0] + \" \")\n",
    "        if word[1] in [\"Noun\"]:\n",
    "            writeFp.write(word[0] + \" \")\n",
    "writeFp.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
